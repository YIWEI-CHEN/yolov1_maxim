{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO v1 on MAX78000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook covers the MAX78000 model training and synthesis pipeline for the YOLO v1 model.\n",
    "\n",
    "Prerequisites:\n",
    " - The `ai8x-training` folder must be located underneath a common root directory. Training and Synthesis repositories must be initialized and ready to use (please see [README.md](README.md)).\n",
    " - This notebook must be located inside the `ai8x-training` folder.\n",
    " - Place `yolo` folder into `ai8x-training`.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "import os\n",
    "from matplotlib.image import imread\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import distiller.apputils as apputils\n",
    "import cv2\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, 'yolo/')\n",
    "sys.path.insert(1, 'distiller/')\n",
    "sys.path.insert(2, '/data/detection/')\n",
    "\n",
    "from YOLO_V1_DataSet_small import YoloV1DataSet\n",
    "from YOLO_V1_LossFunction import  Yolov1_Loss\n",
    "\n",
    "mod = importlib.import_module(\"yolov1_bn_model_noaffine\")\n",
    "\n",
    "import ai8x\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I - Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_params(model):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self, act_mode_8bit):\n",
    "        self.act_mode_8bit = act_mode_8bit\n",
    "        self.truncate_testset = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data path, modify to match file system layout\n",
    "data_path = '/data/detection'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Unique images 486\n",
      "486 486\n",
      "{'person': 0, 'car': 1, 'bicycle': 2, 'chair': 3, 'sofa': 4}\n"
     ]
    }
   ],
   "source": [
    "dataSet = YoloV1DataSet(imgs_dir=\"/data/detection/dataset/train/VOC2007/Train/JPEGImages\",\n",
    "                        annotations_dir=\"/data/detection/dataset/train/VOC2007/Train/Annotations\",\n",
    "                        ClassesFile=\"/data/detection/dataset/train/VOC2007/Train/VOC_remain_class.data\",\n",
    "                        data_path=data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataLoader = DataLoader(dataSet,batch_size=16,shuffle=True,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda:0\n",
      "Configuring device: MAX78000, simulate=False.\n",
      "YOLO V1 Model_Z %d class (224 input), %d bounding boxes.\n",
      "NUMBER OF PARAMETERS 296591\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Running on device: {}'.format(device))\n",
    "\n",
    "ai8x.set_device(device=85, simulate=False, round_avg=False)\n",
    "Yolo = mod.Yolov1_net(num_classes=dataSet.Classes, bias=True)\n",
    "Yolo = Yolo.to(device)\n",
    "print(\"NUMBER OF PARAMETERS\",  sum(p.numel() for p in Yolo.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = Yolov1_Loss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(Yolo.parameters(),lr=3e-5,momentum=0.9,weight_decay=0.0005)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,milestones=[50, 100,200,300,400,500,600,700,800,900,1000,2000,3000,4000,5000,6000,7000,8000,9000,10000,20000,30000,40000],gamma=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 400\n",
    "qat_policy = {'start_epoch':150,\n",
    "              'weight_bits':8,\n",
    "              'bias_bits':8,\n",
    "              'shift_quantile': 0.99}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0 ; loss : {11724.577575683594} ; avg_loss: {390.8192525227865}\n",
      "epoch : 1 ; loss : {9242.66722869873} ; avg_loss: {308.088907623291}\n",
      "epoch : 2 ; loss : {8296.884826660156} ; avg_loss: {276.5628275553385}\n",
      "epoch : 3 ; loss : {7883.095397949219} ; avg_loss: {262.7698465983073}\n",
      "epoch : 4 ; loss : {7646.734916687012} ; avg_loss: {254.89116388956705}\n",
      "epoch : 5 ; loss : {7468.997764587402} ; avg_loss: {248.96659215291342}\n",
      "epoch : 6 ; loss : {7296.804267883301} ; avg_loss: {243.22680892944337}\n",
      "epoch : 7 ; loss : {7161.190208435059} ; avg_loss: {238.70634028116862}\n",
      "epoch : 8 ; loss : {6985.4044189453125} ; avg_loss: {232.84681396484376}\n",
      "epoch : 9 ; loss : {6830.99479675293} ; avg_loss: {227.699826558431}\n",
      "epoch : 10 ; loss : {6681.527252197266} ; avg_loss: {222.7175750732422}\n",
      "epoch : 11 ; loss : {6450.0129470825195} ; avg_loss: {215.00043156941732}\n",
      "epoch : 12 ; loss : {6210.836292266846} ; avg_loss: {207.02787640889485}\n",
      "epoch : 13 ; loss : {6101.955642700195} ; avg_loss: {203.39852142333984}\n",
      "epoch : 14 ; loss : {5920.621841430664} ; avg_loss: {197.35406138102215}\n",
      "epoch : 15 ; loss : {5716.9865798950195} ; avg_loss: {190.56621932983398}\n",
      "epoch : 16 ; loss : {5617.059509277344} ; avg_loss: {187.23531697591145}\n",
      "epoch : 17 ; loss : {5453.468746185303} ; avg_loss: {181.78229153951008}\n",
      "epoch : 18 ; loss : {5240.197799682617} ; avg_loss: {174.6732599894206}\n",
      "epoch : 19 ; loss : {5129.400318145752} ; avg_loss: {170.9800106048584}\n",
      "epoch : 20 ; loss : {4948.614158630371} ; avg_loss: {164.95380528767905}\n",
      "epoch : 21 ; loss : {4797.008460998535} ; avg_loss: {159.9002820332845}\n",
      "epoch : 22 ; loss : {4663.494136810303} ; avg_loss: {155.44980456034344}\n",
      "epoch : 23 ; loss : {4565.8822021484375} ; avg_loss: {152.19607340494792}\n",
      "epoch : 24 ; loss : {4430.631050109863} ; avg_loss: {147.68770167032878}\n",
      "epoch : 25 ; loss : {4323.047206878662} ; avg_loss: {144.10157356262206}\n",
      "epoch : 26 ; loss : {4188.9196434021} ; avg_loss: {139.63065478007}\n",
      "epoch : 27 ; loss : {4049.417407989502} ; avg_loss: {134.98058026631674}\n",
      "epoch : 28 ; loss : {3928.0504035949707} ; avg_loss: {130.93501345316568}\n",
      "epoch : 29 ; loss : {3825.555498123169} ; avg_loss: {127.51851660410563}\n",
      "epoch : 30 ; loss : {3739.7637252807617} ; avg_loss: {124.65879084269206}\n",
      "epoch : 31 ; loss : {3623.672824859619} ; avg_loss: {120.78909416198731}\n",
      "epoch : 32 ; loss : {3521.4586181640625} ; avg_loss: {117.38195393880208}\n",
      "epoch : 33 ; loss : {3416.712574005127} ; avg_loss: {113.89041913350424}\n",
      "epoch : 34 ; loss : {3294.8961601257324} ; avg_loss: {109.82987200419107}\n",
      "epoch : 35 ; loss : {3167.351650238037} ; avg_loss: {105.57838834126791}\n",
      "epoch : 36 ; loss : {3084.5974674224854} ; avg_loss: {102.81991558074951}\n",
      "epoch : 37 ; loss : {3056.7346267700195} ; avg_loss: {101.89115422566732}\n",
      "epoch : 38 ; loss : {2993.6717414855957} ; avg_loss: {99.78905804951985}\n",
      "epoch : 39 ; loss : {2947.6037254333496} ; avg_loss: {98.25345751444499}\n",
      "epoch : 40 ; loss : {2912.9281425476074} ; avg_loss: {97.09760475158691}\n",
      "epoch : 41 ; loss : {2858.126365661621} ; avg_loss: {95.27087885538737}\n",
      "epoch : 42 ; loss : {2850.272663116455} ; avg_loss: {95.0090887705485}\n",
      "epoch : 43 ; loss : {2790.3440227508545} ; avg_loss: {93.01146742502848}\n",
      "epoch : 44 ; loss : {2729.6297569274902} ; avg_loss: {90.98765856424967}\n",
      "epoch : 45 ; loss : {2667.7276725769043} ; avg_loss: {88.92425575256348}\n",
      "epoch : 46 ; loss : {2632.145206451416} ; avg_loss: {87.73817354838053}\n",
      "epoch : 47 ; loss : {2570.9448585510254} ; avg_loss: {85.69816195170084}\n",
      "epoch : 48 ; loss : {2529.9259147644043} ; avg_loss: {84.33086382548014}\n",
      "epoch : 49 ; loss : {2451.2154121398926} ; avg_loss: {81.70718040466309}\n",
      "epoch : 50 ; loss : {2369.4243354797363} ; avg_loss: {78.98081118265787}\n",
      "epoch : 51 ; loss : {2348.507553100586} ; avg_loss: {78.28358510335286}\n",
      "epoch : 52 ; loss : {2305.9033126831055} ; avg_loss: {76.86344375610352}\n",
      "epoch : 53 ; loss : {2229.811534881592} ; avg_loss: {74.32705116271973}\n",
      "epoch : 54 ; loss : {2171.741533279419} ; avg_loss: {72.3913844426473}\n",
      "epoch : 55 ; loss : {2118.1080169677734} ; avg_loss: {70.60360056559244}\n",
      "epoch : 56 ; loss : {2032.836763381958} ; avg_loss: {67.76122544606527}\n",
      "epoch : 57 ; loss : {1991.4495258331299} ; avg_loss: {66.38165086110433}\n",
      "epoch : 58 ; loss : {1943.44868850708} ; avg_loss: {64.781622950236}\n",
      "epoch : 59 ; loss : {1883.9847240447998} ; avg_loss: {62.79949080149333}\n",
      "epoch : 60 ; loss : {1832.4645347595215} ; avg_loss: {61.08215115865072}\n",
      "epoch : 61 ; loss : {1764.0835552215576} ; avg_loss: {58.80278517405192}\n",
      "epoch : 62 ; loss : {1748.410327911377} ; avg_loss: {58.280344263712564}\n",
      "epoch : 63 ; loss : {1757.1755561828613} ; avg_loss: {58.572518539428714}\n",
      "epoch : 64 ; loss : {1704.3763313293457} ; avg_loss: {56.812544377644855}\n",
      "epoch : 65 ; loss : {1634.3211755752563} ; avg_loss: {54.47737251917521}\n",
      "epoch : 66 ; loss : {1600.5606136322021} ; avg_loss: {53.35202045440674}\n",
      "epoch : 67 ; loss : {1563.6900272369385} ; avg_loss: {52.123000907897946}\n",
      "epoch : 68 ; loss : {1530.0646362304688} ; avg_loss: {51.00215454101563}\n",
      "epoch : 69 ; loss : {1490.7372741699219} ; avg_loss: {49.69124247233073}\n",
      "epoch : 70 ; loss : {1448.661003112793} ; avg_loss: {48.288700103759766}\n",
      "epoch : 71 ; loss : {1412.4505157470703} ; avg_loss: {47.08168385823568}\n",
      "epoch : 72 ; loss : {1378.7788743972778} ; avg_loss: {45.9592958132426}\n",
      "epoch : 73 ; loss : {1337.9148445129395} ; avg_loss: {44.59716148376465}\n",
      "epoch : 74 ; loss : {1315.0650100708008} ; avg_loss: {43.83550033569336}\n",
      "epoch : 75 ; loss : {1302.6286325454712} ; avg_loss: {43.420954418182376}\n",
      "epoch : 76 ; loss : {1284.4731140136719} ; avg_loss: {42.8157704671224}\n",
      "epoch : 77 ; loss : {1242.933175086975} ; avg_loss: {41.431105836232504}\n",
      "epoch : 78 ; loss : {1202.6691970825195} ; avg_loss: {40.08897323608399}\n",
      "epoch : 79 ; loss : {1176.2123699188232} ; avg_loss: {39.20707899729411}\n",
      "epoch : 80 ; loss : {1156.2273893356323} ; avg_loss: {38.54091297785441}\n",
      "epoch : 81 ; loss : {1122.8380479812622} ; avg_loss: {37.42793493270874}\n",
      "epoch : 82 ; loss : {1114.6710481643677} ; avg_loss: {37.155701605478924}\n",
      "epoch : 83 ; loss : {1124.3072872161865} ; avg_loss: {37.47690957387288}\n",
      "epoch : 84 ; loss : {1076.8183288574219} ; avg_loss: {35.893944295247394}\n",
      "epoch : 85 ; loss : {1057.7286701202393} ; avg_loss: {35.25762233734131}\n",
      "epoch : 86 ; loss : {1037.2984504699707} ; avg_loss: {34.57661501566569}\n",
      "epoch : 87 ; loss : {994.2798557281494} ; avg_loss: {33.14266185760498}\n",
      "epoch : 88 ; loss : {1005.4935779571533} ; avg_loss: {33.516452598571775}\n",
      "epoch : 89 ; loss : {992.585147857666} ; avg_loss: {33.086171595255536}\n",
      "epoch : 90 ; loss : {958.8723750114441} ; avg_loss: {31.96241250038147}\n",
      "epoch : 91 ; loss : {936.1478538513184} ; avg_loss: {31.20492846171061}\n",
      "epoch : 92 ; loss : {910.0470504760742} ; avg_loss: {30.334901682535808}\n",
      "epoch : 93 ; loss : {884.1042852401733} ; avg_loss: {29.47014284133911}\n",
      "epoch : 94 ; loss : {849.1016368865967} ; avg_loss: {28.30338789621989}\n",
      "epoch : 95 ; loss : {847.4225053787231} ; avg_loss: {28.24741684595744}\n",
      "epoch : 96 ; loss : {821.0346689224243} ; avg_loss: {27.367822297414143}\n",
      "epoch : 97 ; loss : {785.7805776596069} ; avg_loss: {26.192685921986897}\n",
      "epoch : 98 ; loss : {757.6322650909424} ; avg_loss: {25.254408836364746}\n",
      "epoch : 99 ; loss : {782.4702730178833} ; avg_loss: {26.082342433929444}\n",
      "epoch : 100 ; loss : {726.9295711517334} ; avg_loss: {24.23098570505778}\n",
      "epoch : 101 ; loss : {701.3812131881714} ; avg_loss: {23.379373772939047}\n",
      "epoch : 102 ; loss : {671.227445602417} ; avg_loss: {22.374248186747234}\n",
      "epoch : 103 ; loss : {659.1320114135742} ; avg_loss: {21.971067047119142}\n",
      "epoch : 104 ; loss : {641.5329384803772} ; avg_loss: {21.38443128267924}\n",
      "epoch : 105 ; loss : {634.849289894104} ; avg_loss: {21.161642996470132}\n",
      "epoch : 106 ; loss : {636.838876247406} ; avg_loss: {21.2279625415802}\n",
      "epoch : 107 ; loss : {639.6662540435791} ; avg_loss: {21.322208468119303}\n",
      "epoch : 108 ; loss : {606.0944137573242} ; avg_loss: {20.20314712524414}\n",
      "epoch : 109 ; loss : {594.5725612640381} ; avg_loss: {19.819085375467935}\n",
      "epoch : 110 ; loss : {592.9585599899292} ; avg_loss: {19.76528533299764}\n",
      "epoch : 111 ; loss : {572.5172929763794} ; avg_loss: {19.083909765879312}\n",
      "epoch : 112 ; loss : {558.158929347992} ; avg_loss: {18.605297644933064}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 113 ; loss : {549.1962213516235} ; avg_loss: {18.306540711720785}\n",
      "epoch : 114 ; loss : {534.798632144928} ; avg_loss: {17.8266210714976}\n",
      "epoch : 115 ; loss : {543.9386367797852} ; avg_loss: {18.131287892659504}\n",
      "epoch : 116 ; loss : {524.4820432662964} ; avg_loss: {17.482734775543214}\n",
      "epoch : 117 ; loss : {505.5712580680847} ; avg_loss: {16.852375268936157}\n",
      "epoch : 118 ; loss : {507.30362701416016} ; avg_loss: {16.910120900472005}\n",
      "epoch : 119 ; loss : {495.53395891189575} ; avg_loss: {16.517798630396523}\n",
      "epoch : 120 ; loss : {493.3015456199646} ; avg_loss: {16.44338485399882}\n",
      "epoch : 121 ; loss : {479.87072467803955} ; avg_loss: {15.995690822601318}\n",
      "epoch : 122 ; loss : {475.9064989089966} ; avg_loss: {15.86354996363322}\n",
      "epoch : 123 ; loss : {467.34638118743896} ; avg_loss: {15.578212706247966}\n",
      "epoch : 124 ; loss : {475.29113149642944} ; avg_loss: {15.843037716547649}\n",
      "epoch : 125 ; loss : {469.0866403579712} ; avg_loss: {15.636221345265707}\n",
      "epoch : 126 ; loss : {446.92845249176025} ; avg_loss: {14.897615083058675}\n",
      "epoch : 127 ; loss : {445.70138216018677} ; avg_loss: {14.856712738672892}\n",
      "epoch : 128 ; loss : {438.1865954399109} ; avg_loss: {14.60621984799703}\n",
      "epoch : 129 ; loss : {434.84994316101074} ; avg_loss: {14.494998105367024}\n",
      "epoch : 130 ; loss : {429.786253452301} ; avg_loss: {14.326208448410034}\n",
      "epoch : 131 ; loss : {425.7001085281372} ; avg_loss: {14.190003617604573}\n",
      "epoch : 132 ; loss : {418.8951964378357} ; avg_loss: {13.963173214594523}\n",
      "epoch : 133 ; loss : {418.3190288543701} ; avg_loss: {13.943967628479005}\n",
      "epoch : 134 ; loss : {416.2935404777527} ; avg_loss: {13.876451349258422}\n",
      "epoch : 135 ; loss : {400.26924324035645} ; avg_loss: {13.342308108011881}\n",
      "epoch : 136 ; loss : {401.53705167770386} ; avg_loss: {13.384568389256795}\n",
      "epoch : 137 ; loss : {394.61651515960693} ; avg_loss: {13.153883838653565}\n",
      "epoch : 138 ; loss : {395.0512361526489} ; avg_loss: {13.16837453842163}\n",
      "epoch : 139 ; loss : {389.85216331481934} ; avg_loss: {12.995072110493977}\n",
      "epoch : 140 ; loss : {379.28943967819214} ; avg_loss: {12.642981322606405}\n",
      "epoch : 141 ; loss : {381.1318106651306} ; avg_loss: {12.704393688837687}\n",
      "epoch : 142 ; loss : {365.4910349845886} ; avg_loss: {12.183034499486288}\n",
      "epoch : 143 ; loss : {370.08953857421875} ; avg_loss: {12.336317952473959}\n",
      "epoch : 144 ; loss : {379.31537914276123} ; avg_loss: {12.643845971425375}\n",
      "epoch : 145 ; loss : {364.9475541114807} ; avg_loss: {12.164918470382691}\n",
      "epoch : 146 ; loss : {363.99005126953125} ; avg_loss: {12.133001708984375}\n",
      "epoch : 147 ; loss : {349.60540294647217} ; avg_loss: {11.653513431549072}\n",
      "epoch : 148 ; loss : {343.8260226249695} ; avg_loss: {11.460867420832317}\n",
      "epoch : 149 ; loss : {347.7322688102722} ; avg_loss: {11.591075627009074}\n",
      "QAT is starting!\n",
      "epoch : 150 ; loss : {6471.992485046387} ; avg_loss: {215.73308283487955}\n",
      "epoch : 151 ; loss : {4698.946640014648} ; avg_loss: {156.63155466715494}\n",
      "epoch : 152 ; loss : {3752.081485748291} ; avg_loss: {125.06938285827637}\n",
      "epoch : 153 ; loss : {2838.543270111084} ; avg_loss: {94.6181090037028}\n",
      "epoch : 154 ; loss : {2325.159740447998} ; avg_loss: {77.50532468159993}\n",
      "epoch : 155 ; loss : {2047.6418571472168} ; avg_loss: {68.25472857157389}\n",
      "epoch : 156 ; loss : {1839.8287887573242} ; avg_loss: {61.32762629191081}\n",
      "epoch : 157 ; loss : {1683.364995956421} ; avg_loss: {56.112166531880696}\n",
      "epoch : 158 ; loss : {1567.0662364959717} ; avg_loss: {52.23554121653239}\n",
      "epoch : 159 ; loss : {1469.6746740341187} ; avg_loss: {48.98915580113729}\n",
      "epoch : 160 ; loss : {1388.3872509002686} ; avg_loss: {46.27957503000895}\n",
      "epoch : 161 ; loss : {1324.0545120239258} ; avg_loss: {44.13515040079753}\n",
      "epoch : 162 ; loss : {1269.3527173995972} ; avg_loss: {42.31175724665324}\n",
      "epoch : 163 ; loss : {1216.0347995758057} ; avg_loss: {40.53449331919352}\n",
      "epoch : 164 ; loss : {1171.5252599716187} ; avg_loss: {39.05084199905396}\n",
      "epoch : 165 ; loss : {1132.2691841125488} ; avg_loss: {37.74230613708496}\n",
      "epoch : 166 ; loss : {1100.7677974700928} ; avg_loss: {36.69225991566976}\n",
      "epoch : 167 ; loss : {1065.833508491516} ; avg_loss: {35.52778361638387}\n",
      "epoch : 168 ; loss : {1036.9269361495972} ; avg_loss: {34.56423120498657}\n",
      "epoch : 169 ; loss : {1007.8781633377075} ; avg_loss: {33.595938777923585}\n",
      "epoch : 170 ; loss : {982.1072406768799} ; avg_loss: {32.736908022562666}\n",
      "epoch : 171 ; loss : {961.6241683959961} ; avg_loss: {32.0541389465332}\n",
      "epoch : 172 ; loss : {939.820990562439} ; avg_loss: {31.3273663520813}\n",
      "epoch : 173 ; loss : {920.4833736419678} ; avg_loss: {30.682779121398926}\n",
      "epoch : 174 ; loss : {904.1889562606812} ; avg_loss: {30.139631875356038}\n",
      "epoch : 175 ; loss : {881.624960899353} ; avg_loss: {29.3874986966451}\n",
      "epoch : 176 ; loss : {866.2472114562988} ; avg_loss: {28.874907048543296}\n",
      "epoch : 177 ; loss : {853.7985420227051} ; avg_loss: {28.459951400756836}\n",
      "epoch : 178 ; loss : {836.9408116340637} ; avg_loss: {27.89802705446879}\n",
      "epoch : 179 ; loss : {818.8966579437256} ; avg_loss: {27.296555264790854}\n",
      "epoch : 180 ; loss : {806.181809425354} ; avg_loss: {26.872726980845133}\n",
      "epoch : 181 ; loss : {795.5086517333984} ; avg_loss: {26.51695505777995}\n",
      "epoch : 182 ; loss : {783.2224464416504} ; avg_loss: {26.107414881388348}\n",
      "epoch : 183 ; loss : {768.512246131897} ; avg_loss: {25.61707487106323}\n",
      "epoch : 184 ; loss : {756.5610389709473} ; avg_loss: {25.218701299031576}\n",
      "epoch : 185 ; loss : {743.9154348373413} ; avg_loss: {24.797181161244712}\n",
      "epoch : 186 ; loss : {733.2765669822693} ; avg_loss: {24.442552232742308}\n",
      "epoch : 187 ; loss : {722.1532201766968} ; avg_loss: {24.071774005889893}\n",
      "epoch : 188 ; loss : {713.509036064148} ; avg_loss: {23.7836345354716}\n",
      "epoch : 189 ; loss : {703.7721009254456} ; avg_loss: {23.459070030848185}\n",
      "epoch : 190 ; loss : {695.0505695343018} ; avg_loss: {23.168352317810058}\n",
      "epoch : 191 ; loss : {686.4397583007812} ; avg_loss: {22.88132527669271}\n",
      "epoch : 192 ; loss : {678.6818370819092} ; avg_loss: {22.622727902730308}\n",
      "epoch : 193 ; loss : {669.0614185333252} ; avg_loss: {22.302047284444175}\n",
      "epoch : 194 ; loss : {661.7745451927185} ; avg_loss: {22.05915150642395}\n",
      "epoch : 195 ; loss : {652.7067313194275} ; avg_loss: {21.756891043980918}\n",
      "epoch : 196 ; loss : {645.3737955093384} ; avg_loss: {21.51245985031128}\n",
      "epoch : 197 ; loss : {636.9880075454712} ; avg_loss: {21.23293358484904}\n",
      "epoch : 198 ; loss : {631.049102306366} ; avg_loss: {21.034970076878867}\n",
      "epoch : 199 ; loss : {631.140636920929} ; avg_loss: {21.03802123069763}\n",
      "epoch : 200 ; loss : {618.5111885070801} ; avg_loss: {20.61703961690267}\n",
      "epoch : 201 ; loss : {610.9272019863129} ; avg_loss: {20.364240066210428}\n",
      "epoch : 202 ; loss : {604.0892691612244} ; avg_loss: {20.136308972040812}\n",
      "epoch : 203 ; loss : {598.1819558143616} ; avg_loss: {19.939398527145386}\n",
      "epoch : 204 ; loss : {593.2446413040161} ; avg_loss: {19.77482137680054}\n",
      "epoch : 205 ; loss : {589.072934627533} ; avg_loss: {19.63576448758443}\n",
      "epoch : 206 ; loss : {584.0359988212585} ; avg_loss: {19.467866627375283}\n",
      "epoch : 207 ; loss : {577.7230577468872} ; avg_loss: {19.257435258229574}\n",
      "epoch : 208 ; loss : {573.3168931007385} ; avg_loss: {19.11056310335795}\n",
      "epoch : 209 ; loss : {568.218578338623} ; avg_loss: {18.940619277954102}\n",
      "epoch : 210 ; loss : {563.4640808105469} ; avg_loss: {18.78213602701823}\n",
      "epoch : 211 ; loss : {560.8489899635315} ; avg_loss: {18.694966332117716}\n",
      "epoch : 212 ; loss : {556.2813649177551} ; avg_loss: {18.54271216392517}\n",
      "epoch : 213 ; loss : {551.0821723937988} ; avg_loss: {18.36940574645996}\n",
      "epoch : 214 ; loss : {546.7991986274719} ; avg_loss: {18.226639954249062}\n",
      "epoch : 215 ; loss : {541.9036026000977} ; avg_loss: {18.063453420003256}\n",
      "epoch : 216 ; loss : {537.4024882316589} ; avg_loss: {17.91341627438863}\n",
      "epoch : 217 ; loss : {534.1323261260986} ; avg_loss: {17.804410870869955}\n",
      "epoch : 218 ; loss : {530.1273756027222} ; avg_loss: {17.670912520090738}\n",
      "epoch : 219 ; loss : {525.7032470703125} ; avg_loss: {17.523441569010416}\n",
      "epoch : 220 ; loss : {521.5108652114868} ; avg_loss: {17.38369550704956}\n",
      "epoch : 221 ; loss : {517.5196418762207} ; avg_loss: {17.25065472920736}\n",
      "epoch : 222 ; loss : {514.0972690582275} ; avg_loss: {17.13657563527425}\n",
      "epoch : 223 ; loss : {511.04770612716675} ; avg_loss: {17.034923537572226}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 224 ; loss : {509.20156383514404} ; avg_loss: {16.973385461171468}\n",
      "epoch : 225 ; loss : {504.0981831550598} ; avg_loss: {16.803272771835328}\n",
      "epoch : 226 ; loss : {500.4142212867737} ; avg_loss: {16.680474042892456}\n",
      "epoch : 227 ; loss : {496.92449378967285} ; avg_loss: {16.564149792989095}\n",
      "epoch : 228 ; loss : {494.4205515384674} ; avg_loss: {16.480685051282247}\n",
      "epoch : 229 ; loss : {489.93157386779785} ; avg_loss: {16.33105246225993}\n",
      "epoch : 230 ; loss : {485.75095415115356} ; avg_loss: {16.19169847170512}\n",
      "epoch : 231 ; loss : {481.471875667572} ; avg_loss: {16.049062522252402}\n",
      "epoch : 232 ; loss : {477.7222566604614} ; avg_loss: {15.92407522201538}\n",
      "epoch : 233 ; loss : {474.3010640144348} ; avg_loss: {15.810035467147827}\n",
      "epoch : 234 ; loss : {495.3179588317871} ; avg_loss: {16.510598627726235}\n",
      "epoch : 235 ; loss : {503.94938468933105} ; avg_loss: {16.798312822977703}\n",
      "epoch : 236 ; loss : {494.9067325592041} ; avg_loss: {16.496891085306803}\n",
      "epoch : 237 ; loss : {485.6262640953064} ; avg_loss: {16.187542136510213}\n",
      "epoch : 238 ; loss : {478.3240385055542} ; avg_loss: {15.944134616851807}\n",
      "epoch : 239 ; loss : {472.25479221343994} ; avg_loss: {15.741826407114665}\n",
      "epoch : 240 ; loss : {466.0029900074005} ; avg_loss: {15.533433000246683}\n",
      "epoch : 241 ; loss : {460.3141942024231} ; avg_loss: {15.343806473414103}\n",
      "epoch : 242 ; loss : {454.2128291130066} ; avg_loss: {15.14042763710022}\n",
      "epoch : 243 ; loss : {447.2773766517639} ; avg_loss: {14.90924588839213}\n",
      "epoch : 244 ; loss : {443.17284870147705} ; avg_loss: {14.772428290049236}\n",
      "epoch : 245 ; loss : {438.7320261001587} ; avg_loss: {14.62440087000529}\n",
      "epoch : 246 ; loss : {433.4350814819336} ; avg_loss: {14.447836049397786}\n",
      "epoch : 247 ; loss : {429.16859674453735} ; avg_loss: {14.305619891484579}\n",
      "epoch : 248 ; loss : {423.81246614456177} ; avg_loss: {14.127082204818725}\n",
      "epoch : 249 ; loss : {419.54456615448} ; avg_loss: {13.984818871816}\n",
      "epoch : 250 ; loss : {416.9111433029175} ; avg_loss: {13.897038110097249}\n",
      "epoch : 251 ; loss : {412.8456974029541} ; avg_loss: {13.761523246765137}\n",
      "epoch : 252 ; loss : {409.1120114326477} ; avg_loss: {13.637067047754924}\n",
      "epoch : 253 ; loss : {406.1528663635254} ; avg_loss: {13.53842887878418}\n",
      "epoch : 254 ; loss : {402.9730415344238} ; avg_loss: {13.432434717814127}\n",
      "epoch : 255 ; loss : {397.9884121417999} ; avg_loss: {13.266280404726665}\n",
      "epoch : 256 ; loss : {394.21640253067017} ; avg_loss: {13.140546751022338}\n",
      "epoch : 257 ; loss : {390.78899574279785} ; avg_loss: {13.026299858093262}\n",
      "epoch : 258 ; loss : {387.92575573921204} ; avg_loss: {12.930858524640401}\n",
      "epoch : 259 ; loss : {383.92970514297485} ; avg_loss: {12.797656838099162}\n",
      "epoch : 260 ; loss : {380.76710414886475} ; avg_loss: {12.692236804962159}\n",
      "epoch : 261 ; loss : {376.5930986404419} ; avg_loss: {12.55310328801473}\n",
      "epoch : 262 ; loss : {371.1319308280945} ; avg_loss: {12.371064360936483}\n",
      "epoch : 263 ; loss : {369.031436920166} ; avg_loss: {12.301047897338867}\n",
      "epoch : 264 ; loss : {367.67906665802} ; avg_loss: {12.255968888600668}\n",
      "epoch : 265 ; loss : {366.11311864852905} ; avg_loss: {12.203770621617634}\n",
      "epoch : 266 ; loss : {364.1708369255066} ; avg_loss: {12.139027897516886}\n",
      "epoch : 267 ; loss : {361.49339628219604} ; avg_loss: {12.0497798760732}\n",
      "epoch : 268 ; loss : {357.346586227417} ; avg_loss: {11.911552874247233}\n",
      "epoch : 269 ; loss : {355.3128309249878} ; avg_loss: {11.843761030832926}\n",
      "epoch : 270 ; loss : {353.61185359954834} ; avg_loss: {11.787061786651611}\n",
      "epoch : 271 ; loss : {350.12292671203613} ; avg_loss: {11.670764223734539}\n",
      "epoch : 272 ; loss : {346.1508278846741} ; avg_loss: {11.538360929489135}\n",
      "epoch : 273 ; loss : {343.2593069076538} ; avg_loss: {11.441976896921794}\n",
      "epoch : 274 ; loss : {340.9122884273529} ; avg_loss: {11.36374294757843}\n",
      "epoch : 275 ; loss : {339.1423327922821} ; avg_loss: {11.304744426409403}\n",
      "epoch : 276 ; loss : {335.38343715667725} ; avg_loss: {11.179447905222576}\n",
      "epoch : 277 ; loss : {332.7371737957001} ; avg_loss: {11.091239126523336}\n",
      "epoch : 278 ; loss : {330.45894956588745} ; avg_loss: {11.015298318862914}\n",
      "epoch : 279 ; loss : {329.4162492752075} ; avg_loss: {10.980541642506918}\n",
      "epoch : 280 ; loss : {329.4432899951935} ; avg_loss: {10.981442999839782}\n",
      "epoch : 281 ; loss : {326.2837598323822} ; avg_loss: {10.876125327746074}\n",
      "epoch : 282 ; loss : {322.4200584888458} ; avg_loss: {10.747335282961528}\n",
      "epoch : 283 ; loss : {320.1432282924652} ; avg_loss: {10.671440943082173}\n",
      "epoch : 284 ; loss : {319.6330418586731} ; avg_loss: {10.654434728622437}\n",
      "epoch : 285 ; loss : {317.9549078941345} ; avg_loss: {10.598496929804485}\n",
      "epoch : 286 ; loss : {314.27542066574097} ; avg_loss: {10.4758473555247}\n",
      "epoch : 287 ; loss : {311.9541549682617} ; avg_loss: {10.39847183227539}\n",
      "epoch : 288 ; loss : {309.5656418800354} ; avg_loss: {10.318854729334513}\n",
      "epoch : 289 ; loss : {307.7477341890335} ; avg_loss: {10.258257806301117}\n",
      "epoch : 290 ; loss : {307.35866117477417} ; avg_loss: {10.245288705825805}\n",
      "epoch : 291 ; loss : {305.53908383846283} ; avg_loss: {10.18463612794876}\n",
      "epoch : 292 ; loss : {305.47182607650757} ; avg_loss: {10.182394202550253}\n",
      "epoch : 293 ; loss : {304.0066957473755} ; avg_loss: {10.133556524912516}\n",
      "epoch : 294 ; loss : {299.8622727394104} ; avg_loss: {9.99540909131368}\n",
      "epoch : 295 ; loss : {297.3954737186432} ; avg_loss: {9.913182457288107}\n",
      "epoch : 296 ; loss : {296.24288964271545} ; avg_loss: {9.874762988090515}\n",
      "epoch : 297 ; loss : {294.507798910141} ; avg_loss: {9.816926630338033}\n",
      "epoch : 298 ; loss : {292.9878029823303} ; avg_loss: {9.76626009941101}\n",
      "epoch : 299 ; loss : {290.81117272377014} ; avg_loss: {9.693705757459005}\n",
      "epoch : 300 ; loss : {288.56367337703705} ; avg_loss: {9.618789112567901}\n",
      "epoch : 301 ; loss : {286.72760248184204} ; avg_loss: {9.557586749394735}\n",
      "epoch : 302 ; loss : {283.59876680374146} ; avg_loss: {9.453292226791381}\n",
      "epoch : 303 ; loss : {282.1221573352814} ; avg_loss: {9.404071911176045}\n",
      "epoch : 304 ; loss : {280.88879132270813} ; avg_loss: {9.362959710756938}\n",
      "epoch : 305 ; loss : {279.7702875137329} ; avg_loss: {9.325676250457764}\n",
      "epoch : 306 ; loss : {277.62142419815063} ; avg_loss: {9.254047473271688}\n",
      "epoch : 307 ; loss : {277.05220127105713} ; avg_loss: {9.235073375701905}\n",
      "epoch : 308 ; loss : {275.71868658065796} ; avg_loss: {9.190622886021933}\n",
      "epoch : 309 ; loss : {274.66647267341614} ; avg_loss: {9.15554908911387}\n",
      "epoch : 310 ; loss : {273.93067502975464} ; avg_loss: {9.131022500991822}\n",
      "epoch : 311 ; loss : {273.06582164764404} ; avg_loss: {9.102194054921467}\n",
      "epoch : 312 ; loss : {271.63711762428284} ; avg_loss: {9.054570587476094}\n",
      "epoch : 313 ; loss : {269.8583643436432} ; avg_loss: {8.995278811454773}\n",
      "epoch : 314 ; loss : {269.3481442928314} ; avg_loss: {8.978271476427715}\n",
      "epoch : 315 ; loss : {267.6231429576874} ; avg_loss: {8.920771431922912}\n",
      "epoch : 316 ; loss : {266.82209157943726} ; avg_loss: {8.894069719314576}\n",
      "epoch : 317 ; loss : {266.7714955806732} ; avg_loss: {8.89238318602244}\n",
      "epoch : 318 ; loss : {265.4857907295227} ; avg_loss: {8.849526357650756}\n",
      "epoch : 319 ; loss : {264.2360575199127} ; avg_loss: {8.80786858399709}\n",
      "epoch : 320 ; loss : {262.3375587463379} ; avg_loss: {8.744585291544597}\n",
      "epoch : 321 ; loss : {261.2584857940674} ; avg_loss: {8.70861619313558}\n",
      "epoch : 322 ; loss : {260.8650641441345} ; avg_loss: {8.695502138137817}\n",
      "epoch : 323 ; loss : {260.44889736175537} ; avg_loss: {8.681629912058513}\n",
      "epoch : 324 ; loss : {259.098117351532} ; avg_loss: {8.636603911717733}\n",
      "epoch : 325 ; loss : {257.42264103889465} ; avg_loss: {8.580754701296488}\n",
      "epoch : 326 ; loss : {255.65462231636047} ; avg_loss: {8.521820743878683}\n",
      "epoch : 327 ; loss : {254.50508499145508} ; avg_loss: {8.483502833048503}\n",
      "epoch : 328 ; loss : {253.9580795764923} ; avg_loss: {8.46526931921641}\n",
      "epoch : 329 ; loss : {254.11487483978271} ; avg_loss: {8.470495827992757}\n",
      "epoch : 330 ; loss : {253.86241555213928} ; avg_loss: {8.462080518404642}\n",
      "epoch : 331 ; loss : {252.7998423576355} ; avg_loss: {8.426661411921183}\n",
      "epoch : 332 ; loss : {252.11530447006226} ; avg_loss: {8.40384348233541}\n",
      "epoch : 333 ; loss : {250.2696888446808} ; avg_loss: {8.34232296148936}\n",
      "epoch : 334 ; loss : {248.69241189956665} ; avg_loss: {8.289747063318888}\n"
     ]
    }
   ],
   "source": [
    "# MAIN TRAINING\n",
    "\n",
    "best_acc = 0\n",
    "best_qat_acc = 0\n",
    "for epoch in range(0, num_epochs):\n",
    "    loss_sum = 0\n",
    "    loss_coord = 0\n",
    "    loss_confidence = 0\n",
    "    loss_classes = 0\n",
    "    epoch_iou = 0\n",
    "    epoch_object_num = 0\n",
    "    scheduler.step()\n",
    "    \n",
    "    if epoch > 0 and epoch == qat_policy['start_epoch']:\n",
    "        print('QAT is starting!')\n",
    "        # Fuse the BN parameters into conv layers before Quantization Aware Training (QAT)\n",
    "        torch.save(Yolo.state_dict(), f'yolo_models/scaled224_noaffine_shift{qat_policy[\"shift_quantile\"]}_maxim_yolo_beforeQAT_ep{epoch:04d}.pth')\n",
    "        ai8x.fuse_bn_layers(Yolo)\n",
    "\n",
    "        # Switch model from unquantized to quantized for QAT\n",
    "        ai8x.initiate_qat(Yolo, qat_policy)\n",
    "\n",
    "        # Model is re-transferred to GPU in case parameters were added\n",
    "        Yolo.to(device)\n",
    "\n",
    "\n",
    "    for batch_index, batch_train in enumerate(dataLoader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_data = batch_train[0].float().to(device)\n",
    "        train_data.requires_grad = True\n",
    "        \n",
    "        label_data = batch_train[1].float().to(device)\n",
    "        label_data[:, :, :, :, 5] = label_data[:, :, :, :, 5] / 224\n",
    "        label_data[:, :, :, :, 6] = label_data[:, :, :, :, 6] / 224\n",
    "        label_data[:, :, :, :, 7] = label_data[:, :, :, :, 7] / 224\n",
    "        label_data[:, :, :, :, 8] = label_data[:, :, :, :, 8] / 224\n",
    "        label_data[:, :, :, :, 9] = label_data[:, :, :, :, 9] / (224*224)\n",
    "        \n",
    "        \n",
    "#         label_data = batch_train[1].float().to(device)\n",
    "        bb_pred, _ = Yolo(train_data)\n",
    "        loss = loss_function(bounding_boxes=bb_pred,ground_truth=label_data)\n",
    "        batch_loss = loss[0]\n",
    "        loss_coord = loss_coord + loss[1]\n",
    "        loss_confidence = loss_confidence + loss[2]\n",
    "        loss_classes = loss_classes + loss[3]\n",
    "        epoch_iou = epoch_iou + loss[4]\n",
    "        epoch_object_num = epoch_object_num + loss[5]\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_loss = batch_loss.item()\n",
    "        loss_sum = loss_sum + batch_loss\n",
    "        \n",
    "        #print(\"batch_index : {} ; batch_loss : {}\".format(batch_index, batch_loss))\n",
    "    \n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        if epoch >= qat_policy['start_epoch']:\n",
    "            torch.save(Yolo.state_dict(), f'yolo_models/scaled224_noaffine_shift{qat_policy[\"shift_quantile\"]}_maxim_yolo_qat_ep{epoch:04d}.pth')\n",
    "        else:\n",
    "            torch.save(Yolo.state_dict(), f'yolo_models/scaled224_noaffine_shift{qat_policy[\"shift_quantile\"]}_maxim_yolo_ep{epoch:04d}.pth')\n",
    "            \n",
    "        \n",
    "    avg_loss= loss_sum/batch_index\n",
    "    print(\"epoch : {} ; loss : {} ; avg_loss: {}\".format(epoch,{loss_sum},{avg_loss}))\n",
    "    \n",
    "    epoch = epoch + 1\n",
    "    \n",
    "torch.save(Yolo.state_dict(), f'yolo_models/scaled224_noaffine_shift{qat_policy[\"shift_quantile\"]}_maxim_yolo_qat_ep{epoch:04d}.pth')    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
